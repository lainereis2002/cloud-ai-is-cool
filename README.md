# ü§ñ BeeMo - AI Assistant

> Um sistema de chatbot educacional inteligente que utiliza a Gemini API do Google para responder d√∫vidas de forma clara, did√°tica e motivacional.

**Reposit√≥rio dedicado para segunda avalia√ß√£o da disciplina de Cloud.**

---

## üìö Tabela de Conte√∫dos

1. [O que √©?](#1-o-que-√©)
2. [Como Rodar](#2-como-rodar)
3. [Design Pattern](#3-design-pattern)
4. [Containeriza√ß√£o com Render](#4-containeriza√ß√£o-com-render)

---

## 1Ô∏è‚É£ O que √©?

### Vis√£o Geral

O **Assistente Inteligente** √© uma aplica√ß√£o full-stack que combina:

- **Backend**: API REST constru√≠da com **FastAPI** (Python)
- **Frontend**: Interface interativa com **Streamlit** (Python)
- **LLM**: Gemini 2.5 Flash do Google para gerar respostas inteligentes

### Funcionalidades Principais

‚úÖ **Chat em Tempo Real** - Intera√ß√£o fluida entre usu√°rio e assistente  
‚úÖ **Respostas Personalizadas** - System instructions customizadas para educa√ß√£o  
‚úÖ **CORS Habilitado** - Comunica√ß√£o entre frontend e backend sem restri√ß√µes  
‚úÖ **Tratamento Robusto de Erros** - Feedback claro ao usu√°rio  
‚úÖ **Health Check** - Verifica√ß√£o da sa√∫de da API  
‚úÖ **Logging Detalhado** - Debug facilitado em produ√ß√£o  

### Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Frontend (Streamlit)             ‚îÇ
‚îÇ              Interface Web Interativa                ‚îÇ
‚îÇ           (http://localhost:8501)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                HTTP POST /chat
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Backend (FastAPI)                      ‚îÇ
‚îÇ          API REST (http://localhost:8000)          ‚îÇ
‚îÇ    - Health Check: GET /                           ‚îÇ
‚îÇ    - Chat: POST /chat                              ‚îÇ
‚îÇ    - Debug: GET /debug/config                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
              API Gemini v1beta
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Google Gemini API (LLM)                     ‚îÇ
‚îÇ      Processamento de Linguagem Natural             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Stack Tecnol√≥gico

| Componente | Tecnologia | Vers√£o |
|-----------|-----------|--------|
| **Runtime** | Python | 3.9+ |
| **Backend** | FastAPI | 0.123.0+ |
| **Frontend** | Streamlit | 1.28.0+ |
| **LLM** | google-genai | 1.47.0+ |
| **Server** | Uvicorn | 0.23.0+ |
| **CORS** | fastapi.middleware | Integrado |
| **Environment** | python-dotenv | 1.0.0+ |

---

## 2Ô∏è‚É£ Como Rodar

### Pr√©-requisitos

- **Python 3.9+** instalado
- **pip** para gerenciamento de pacotes
- **Chave de API do Google Gemini** (gratuita em [AI Studio](https://aistudio.google.com/))
- **Git** (opcional, para clonar o reposit√≥rio)

### Instala√ß√£o Local

#### Passo 1: Clone o reposit√≥rio
```bash
git clone https://github.com/lainereis2002/cloud-ai-is-cool.git
cd cloud-ai-is-cool
```

#### Passo 2: Crie um ambiente virtual
```bash
python -m venv venv

# Ative o ambiente virtual
source venv/bin/activate  # macOS/Linux
# ou
venv\Scripts\activate     # Windows
```

#### Passo 3: Instale as depend√™ncias

**Backend:**
```bash
pip install -r requirements.txt
```

**Frontend (em outro terminal):**
```bash
cd frontend
pip install -r requirements.txt
cd ..
```

#### Passo 4: Configure vari√°veis de ambiente

Crie um arquivo `.env` na raiz do projeto:
```env
GEMINI_API_KEY=sua-chave-de-api-aqui
FASTAPI_BACKEND_URL=http://127.0.0.1:8000
```

#### Passo 5: Execute a aplica√ß√£o

**Terminal 1 - Backend (FastAPI):**
```bash
source venv/bin/activate  # ou venv\Scripts\activate no Windows
uvicorn main:app --reload --host 127.0.0.1 --port 8000
```

Sa√≠da esperada:
```
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:main:Cliente Gemini inicializado com sucesso. Modelo: gemini-2.5-flash
INFO:     Application startup complete.
```

**Terminal 2 - Frontend (Streamlit):**
```bash
source venv/bin/activate  # ou venv\Scripts\activate no Windows
cd frontend
streamlit run streamlit_app.py
```

A aplica√ß√£o abrir√° automaticamente em: **http://localhost:8501**

### Testando a API

#### Usando o Script de Teste:
```bash
python test_api.py
```

#### Usando cURL:
```bash
# Health Check
curl http://127.0.0.1:8000/

# Chat
curl -X POST http://127.0.0.1:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "O que √© Cloud Computing?"}'

# Debug Config
curl http://127.0.0.1:8000/debug/config
```

#### Usando Python:
```python
import requests

response = requests.post(
    "http://127.0.0.1:8000/chat",
    json={"message": "Explique Python em uma frase"}
)

print(response.json())
```

### Troubleshooting

| Erro | Solu√ß√£o |
|------|---------|
| `GEMINI_API_KEY n√£o est√° configurada` | Crie `.env` com a chave da API Gemini |
| `Address already in use :8000` | Mude a porta: `--port 8001` ou mate o processo: `lsof -ti:8000 \| xargs kill -9` |
| `Connection refused` | Verifique se FastAPI est√° rodando no Terminal 1 |
| `Timeout na requisi√ß√£o` | Aumente o timeout ou verifique sua cota Gemini |

---

## 3Ô∏è‚É£ Design Pattern

### Padr√µes de Design Implementados

#### üèóÔ∏è **1. MVC (Model-View-Controller)**

```
Model (Pydantic)        View (Streamlit)        Controller (FastAPI)
    ‚Üì                        ‚Üì                          ‚Üì
ChatRequest         UI Interativa          Endpoints REST
Schema JSON         Chat History           L√≥gica Business
Valida√ß√£o           Componentes            Orquestra√ß√£o
```

**Estrutura:**
- **Model**: `ChatRequest` (Pydantic BaseModel) valida entrada do usu√°rio
- **View**: `streamlit_app.py` renderiza interface web
- **Controller**: `main.py` processa requisi√ß√µes e coordena l√≥gica

#### üîå **2. Repository Pattern (Separa√ß√£o de Responsabilidades)**

```
Client (Streamlit)
       ‚Üì
APIRepository (requests.post)
       ‚Üì
FastAPI Endpoint (@app.post)
       ‚Üì
LLMService (Google Gemini)
```

Cada camada tem responsabilidade clara:
- **Client Layer**: UI/Intera√ß√£o com usu√°rio
- **API Layer**: Comunica√ß√£o HTTP
- **Service Layer**: L√≥gica de neg√≥cio
- **LLM Layer**: Integra√ß√£o externa

#### üõ°Ô∏è **3. Middleware Pattern**

```python
app.add_middleware(
    CORSMiddleware,  # Permite comunica√ß√£o Frontend ‚Üî Backend
    allow_origins=["*"],
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"]
)
```

Middleware aplicado para:
- ‚úÖ Habilitar CORS
- ‚úÖ Permitir requisi√ß√µes pr√©-voo (OPTIONS)
- ‚úÖ Validar headers

#### üìù **4. Adapter Pattern (LLM Integration)**

```python
# Interface unificada para diferentes modelos
class LLMAdapter:
    def generate_response(self, prompt: str) -> str:
        # Pode trocar Gemini por GPT, Claude, etc.
        pass
```

Permite trocar provedores LLM facilmente:
```python
# Hoje: Google Gemini
response = client.models.generate_content(...)

# Amanh√£: OpenAI GPT
response = openai.ChatCompletion.create(...)
```

#### üîÑ **5. Singleton Pattern (Client Initialization)**

```python
# Inst√¢ncia √∫nica do cliente Gemini
client = genai.Client(api_key=GEMINI_API_KEY)
MODEL = 'gemini-2.5-flash'

# Reutilizada em todas as requisi√ß√µes
@app.post("/chat")
async def process_chat(request: ChatRequest):
    response = client.models.generate_content(...)
```

Uma √∫nica inst√¢ncia do cliente em toda aplica√ß√£o ‚ú®

#### üéØ **6. Observer Pattern (Error Handling)**

```python
try:
    response = client.models.generate_content(...)
except APIError as e:
    # Observer: Monitora erro espec√≠fico
    if "RESOURCE_EXHAUSTED" in str(e):
        raise HTTPException(status_code=429)
except Exception as e:
    # Observer: Monitora erro gen√©rico
    raise HTTPException(status_code=500)
```

Diferentes observadores reagem a diferentes erros

### Fluxo de Dados

```
1. Usu√°rio digita pergunta no Streamlit
         ‚Üì
2. Streamlit envia POST /chat com {"message": "..."}
         ‚Üì
3. FastAPI recebe e valida com ChatRequest (Pydantic)
         ‚Üì
4. Sistema instructions personalizado √© aplicado
         ‚Üì
5. API Gemini √© chamada com conte√∫do + config
         ‚Üì
6. Resposta √© formatada em JSON
         ‚Üì
7. Streamlit renderiza resposta no chat
         ‚Üì
8. Hist√≥rico √© mantido no session_state do Streamlit
```

### Camadas de Abstra√ß√£o

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Presentation Layer (Streamlit UI)      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Application Layer (FastAPI Routes)     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Business Logic Layer (Chat Processing) ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Integration Layer (Gemini API Client)  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  External Service (Google Gemini API)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 4Ô∏è‚É£ Containeriza√ß√£o com Render

### O que √© Render?

**Render** √© uma plataforma cloud moderna que simplifica o deploy de aplica√ß√µes. √â a alternativa atualizada ao Heroku.

### Arquitetura Docker

Utilizamos **Multi-stage Build** para otimizar a imagem:

```dockerfile
# STAGE 1: Build
FROM python:3.11-slim as builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# STAGE 2: Runtime (imagem final, mais leve)
FROM python:3.11-slim
WORKDIR /app
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY . .
EXPOSE 80
CMD ["python3", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "80"]
```

**Benef√≠cios:**
- ‚úÖ Imagem final **50% mais leve** (builder layers s√£o descartadas)
- ‚úÖ Depend√™ncias instaladas uma s√≥ vez
- ‚úÖ Melhor desempenho no deploy
- ‚úÖ Menos uso de banda de rede

### Deploy no Render - Passo a Passo

#### 1Ô∏è‚É£ **Prepare o reposit√≥rio**

Certifique-se que na raiz tem:
```
cloud-ai-is-cool/
‚îú‚îÄ‚îÄ Dockerfile           ‚úÖ Multi-stage build
‚îú‚îÄ‚îÄ requirements.txt     ‚úÖ Depend√™ncias backend
‚îú‚îÄ‚îÄ main.py             ‚úÖ FastAPI app
‚îú‚îÄ‚îÄ .dockerignore        ‚úÖ (opcional) excluir venv/, .git/
‚îî‚îÄ‚îÄ .env.example        ‚úÖ (opcional) template de env
```

**Crie `.dockerignore` na raiz:**
```
venv/
.git/
.gitignore
__pycache__/
*.pyc
.env
.pytest_cache/
```

#### 2Ô∏è‚É£ **Configure environment no Render**

No painel do Render, configure as vari√°veis:

| Vari√°vel | Valor | Exemplo |
|----------|-------|---------|
| `GEMINI_API_KEY` | Sua chave | `AIzaSyA5JmXW...` |
| `FASTAPI_BACKEND_URL` | URL da API | `https://seu-app.onrender.com` |
| `PORT` | Porta (opcional) | `80` |

#### 3Ô∏è‚É£ **Deploy via Git**

```bash
# Push para GitHub/GitLab/Bitbucket
git add .
git commit -m "Deploy para Render"
git push origin main  # ou sua branch
```

#### 4Ô∏è‚É£ **Crie Web Service no Render**

1. Acesse https://dashboard.render.com
2. Clique em **"New +" ‚Üí "Web Service"**
3. Conecte seu reposit√≥rio Git
4. Configure:
   - **Name**: `cloud-ai-chatbot`
   - **Environment**: `Docker`
   - **Region**: `Ohio` (US) ou pr√≥ximo
   - **Plan**: `Free` (trial) ou `Pay-as-you-go`

5. Clique em **"Deploy"**

#### 5Ô∏è‚É£ **Configure Vari√°veis de Ambiente**

No Render Dashboard:
1. V√° para **Settings ‚Üí Environment**
2. Adicione as vari√°veis:
   ```
   GEMINI_API_KEY=sua-chave-aqui
   FASTAPI_BACKEND_URL=https://seu-app.onrender.com
   ```
3. Clique em **"Save"**

#### 6Ô∏è‚É£ **Deploy Manual (Opcional)**

Se a build falhar, redeploy:
```bash
# No dashboard, clique em "Manual Deploy ‚Üí Deploy latest commit"
```

### Monitoramento no Render

#### Logs da Aplica√ß√£o

```bash
# No dashboard, v√° para "Logs"
# Voc√™ ver√° em tempo real:

INFO:     Uvicorn running on http://0.0.0.0:80
INFO:main:Cliente Gemini inicializado com sucesso
INFO:     Application startup complete
INFO:main:Requisi√ß√£o de chat recebida
```

#### Health Checks

Render faz checks autom√°ticos:

```
GET https://seu-app.onrender.com/
Resposta esperada: {"status":"ok","message":"Assistente de estudos online!"}
```

Se falhar consecutivamente, a app fica em "red" (erro).

### URL Produ√ß√£o

Ap√≥s deploy bem-sucedido:

- **Backend API**: `https://seu-app.onrender.com`
- **Health Check**: `https://seu-app.onrender.com/`
- **Chat Endpoint**: `https://seu-app.onrender.com/chat`

### Deploy do Frontend (Streamlit) no Render

#### M√©todo 1: Usar Community Cloud (Gr√°tis)

1. Commit seu c√≥digo: `git push`
2. Acesse https://share.streamlit.io
3. Conecte seu reposit√≥rio GitHub
4. Aponte para `frontend/streamlit_app.py`
5. Streamlit faz o deploy automaticamente

URL: `https://seu-username-cloud-ai-cool-xxxxx.streamlit.app`

#### M√©todo 2: Web Service Separado (Pago)

1. Crie um `Dockerfile.streamlit` adicional
2. Deploy como outro Web Service no Render
3. Configure `FASTAPI_BACKEND_URL` apontando para seu backend

### Troubleshooting Deploy

| Erro | Solu√ß√£o |
|------|---------|
| `Build failed: pip install` | Atualize `requirements.txt`: `pip freeze > requirements.txt` |
| `GEMINI_API_KEY not found` | Verifique vari√°vel de ambiente no Render Settings |
| `Connection timeout` | Backend pode estar no plano free e ficou dormindo (cold start) |
| `Port already in use` | Dockerfile usa `--port 80`, n√£o mude |

### Estrutura Completa do Deploy

```
GitHub Repository
    ‚Üì
Render Webhook (autom√°tico ao push)
    ‚Üì
Build Stage 1: Download c√≥digo
    ‚Üì
Build Stage 2: Multi-stage Docker build
    ‚Üì
Build Stage 3: Docker push para registro Render
    ‚Üì
Deploy: Container inicia
    ‚Üì
Health Check: GET / passa? ‚úÖ
    ‚Üì
App Online üéâ
    ‚Üì
Streamlit Community Cloud (frontend)
    ‚Üì
Usu√°rios finais acessam:
https://seu-app.onrender.com (API)
https://seu-username-app.streamlit.app (UI)
```

### Exemplo `.env.example` para Render

Crie este arquivo na raiz para documenta√ß√£o:

```env
# Google Gemini API
GEMINI_API_KEY=key

# URLs da Aplica√ß√£o
FASTAPI_BACKEND_URL=https://seu-app.onrender.com
STREAMLIT_SERVER_PORT=80

# FastAPI Config
FASTAPI_HOST=0.0.0.0
FASTAPI_PORT=80
```

---

## üìä Resumo Executivo

| Aspecto | Descri√ß√£o |
|--------|----------|
| **O que √©?** | Assistente com IA (Gemini) que responde d√∫vidas |
| **Como rodar?** | `uvicorn main:app --reload` + `streamlit run streamlit_app.py` |
| **Padr√µes** | MVC, Repository, Middleware, Adapter, Singleton, Observer |
| **Deploy** | Docker multi-stage + Render Web Service |
| **Resultado** | App escal√°vel, robusta e pronta para produ√ß√£o |

---

**Desenvolvido com ‚ù§Ô∏è para a disciplina de Cloud Computing**

