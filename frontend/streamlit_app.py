import streamlit as st
import requests
import os
import json

# --- Configura√ß√£o do Back-end FastAPI ---
FASTAPI_BACKEND_URL = os.getenv("FASTAPI_BACKEND_URL", "http://127.0.0.1:8000") 
CHAT_ENDPOINT = f"{FASTAPI_BACKEND_URL}/chat"

st.set_page_config(
    page_title="Assistente de Estudos Cloud",
    layout="centered"
)

# --- Fun√ß√µes do Chat ---

def get_chatbot_response(prompt: str):
    """Envia a pergunta do usu√°rio para a API do FastAPI e retorna a resposta."""
    try:
        # Define o corpo da requisi√ß√£o JSON conforme o modelo Pydantic do seu FastAPI
        payload = {"message": prompt}
        
        st.write(f"üì° Conectando a: {CHAT_ENDPOINT}")
        
        # Faz a chamada POST para o seu endpoint /chat
        response = requests.post(
            CHAT_ENDPOINT, 
            json=payload, 
            timeout=60,
            headers={"Content-Type": "application/json"}
        )
        
        # Debug: mostra o status code
        st.write(f"Status Code: {response.status_code}")
        
        if response.status_code != 200:
            st.write(f"Resposta completa: {response.text}")
        
        response.raise_for_status()  # Levanta um erro para c√≥digos de status 4xx/5xx

        # Retorna a resposta formatada
        result = response.json()
        return result.get("response", "Erro: Resposta vazia da API.")

    except requests.exceptions.Timeout:
        return "‚è±Ô∏è Erro: A requisi√ß√£o expirou. A API do LLM est√° demorando muito."
    except requests.exceptions.ConnectionError:
        return f"‚ùå Erro de conex√£o: N√£o foi poss√≠vel conectar √† API em {CHAT_ENDPOINT}. Verifique se o servidor FastAPI est√° rodando."
    except requests.exceptions.HTTPError as e:
        try:
            error_detail = e.response.json().get("detail", str(e))
        except:
            error_detail = str(e)
        return f"‚ùå Erro HTTP {e.response.status_code}: {error_detail}"
    except requests.exceptions.RequestException as e:
        return f"‚ùå Erro de conex√£o com o back-end: {e}"
    except json.JSONDecodeError:
        return "‚ùå Erro: A resposta da API n√£o √© um JSON v√°lido."
    except Exception as e:
        return f"‚ùå Erro inesperado: {type(e).__name__}: {e}"


# --- Interface Streamlit ---

st.title("ü§ñ Assistente Inteligente de Estudos em Cloud")
st.caption("Desenvolvido com Python, FastAPI e Gemini API.")

# Mostra URL de debug
with st.expander("üîß Configura√ß√µes de Debug"):
    st.write(f"**URL do Backend:** {FASTAPI_BACKEND_URL}")
    st.write(f"**Endpoint do Chat:** {CHAT_ENDPOINT}")
    
    # Testa conex√£o
    if st.button("Testar Conex√£o"):
        try:
            test_response = requests.get(f"{FASTAPI_BACKEND_URL}/", timeout=5)
            if test_response.status_code == 200:
                st.success("‚úÖ Conex√£o com o backend bem-sucedida!")
                st.write(test_response.json())
            else:
                st.error(f"‚ùå Status code: {test_response.status_code}")
        except Exception as e:
            st.error(f"‚ùå Erro ao conectar: {e}")

# Inicializa o hist√≥rico do chat na session state do Streamlit
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "Ol√°! Eu sou seu assistente em Cloud. Como posso ajudar em seus estudos?"}
    ]

# Exibe o hist√≥rico de mensagens
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# Captura a entrada do usu√°rio
if prompt := st.chat_input("Pergunte-me algo..."):
    # 1. Adiciona a mensagem do usu√°rio ao hist√≥rico
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # 2. Obt√©m a resposta do back-end (FastAPI)
    with st.spinner("‚è≥ Pensando..."):
        full_response = get_chatbot_response(prompt)
    
    # 3. Adiciona a resposta do assistente e a exibe
    st.session_state.messages.append({"role": "assistant", "content": full_response})
    st.chat_message("assistant").write(full_response)